
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm,amsfonts,latexsym,bbm,xspace,graphicx,float,mathtools,
verbatim, xcolor} 
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\newcommand{\new}[1]{\textcolor{red}{#1}}
%\usepackage{psfig}
\usepackage{pgfplots}

\newcommand{\future}[1]{\textcolor{red}{#1}}

\newcommand{\hP}{\hat P}
\newcommand{\hp}{\hat p}

\newcommand{\Dk}{\Delta_k}
\newcommand{\Px}{P(x)}
\newcommand{\Qx}{Q(x)}
\newcommand{\Nx}{N_x}

\newcommand{\Py}{P(y)}
\newcommand{\Qy}{Q(y)}
\newcommand{\Pml}{P_{ML}}
\newcommand{\Pmlx}{\Pml(x)}
\newcommand{\Pbeta}{P_{\beta}}
\newcommand{\Pbetax}{\Pbeta(x)}


\newcommand{\dTV}[2]{d_{TV} (#1,#2)}
\newcommand{\dKL}[2]{D(#1||#2)}
\newcommand{\chisq}[2]{\chi^2(#1,#2)}
\newcommand{\eps}{\varepsilon}

\newcommand{\nPepsp}[1]{n^*(#1, \eps)}
\newcommand{\nPeps}{\nPepsp{\cP}}


\newcommand{\sumX}{\sum_{x\in\cX}}

\newcommand{\Bpr}[1]{Bern(#1)}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\input{../glodef} 

\title{Assignment Ten\\ ECE 4200}
\date{}

\begin{document}
\maketitle 

\begin{itemize}
\item
Provide credit to \textbf{any sources} other than the course staff that helped you solve the problems. This includes \textbf{all students} you talked to regarding the problems. 	
\item
You can look up definitions/basics online (e.g., wikipedia, stack-exchange, etc)
\item
{\bf The due date is 11/29/2020, 23.59.59 eastern time}. 
\item
Submission rules are the same as previous assignments.
\end{itemize}



\begin{problem}{1. (10 points)}
Suppose $W$ is a $k\times d$ matrix, where each entry of $W$ is picked independently from the set $\{-\frac1{\sqrt k},\frac1{\sqrt k}\}$. In other words, for each $i, j$, 
\[
\probof{W_{ij}=-\frac1{\sqrt k}}=\probof{W_{ij}=\frac1{\sqrt k}}=\frac12.
\]

\begin{enumerate}
\item 
Let $\overrightarrow x\in \RR^d$. If we pick $W$ with this distribution, show that
\[
\EE\left[\lVert W\overrightarrow x \rVert_2^2\right] = \lVert \overrightarrow x \rVert_2^2.
\]
\item
Just like the Gaussian matrix we considered in the class, we might as well take a random matrix $W$ designed like this for JL transform. What is an advantage of this matrix over the Gaussian matrix? 
\end{enumerate}
\end{problem}

% Find the eigenvectors and eigenvalues of the matrix
% \[
% \begin{bmatrix}
%     8 & -1 & 2 \\
%     -1 & -2 & 2  \\
%     2 & 2 & -6
% \end{bmatrix}.
% \]
% \end{enumerate}
% \end{problem}

\begin{problem}{2. (10 points)}
Suppose $d=1$. Come up with a set of $n$ real numbers, and an initial set of $k$ distinct cluster centers such that the $k$-means algorithm \textbf{does not converge} to the best solution of the $k$-means clustering problem. You can choose any value of $n$, and $k$ that you want! (Hint: small $n$, $k$ are easier to think about.)
%Consider the function $L = \max\{\sigma(W \overrightarrow x)\}$. Please draw the computational graph for this function, and compute the gradients (which will be Jacobians at some nodes!).
\end{problem}




\begin{problem}{3. (15 points)}
Let $C = \left \lbrace \bar{x}_1 \cdots \bar{x}_{|C|} \right \rbrace$ be a cluster where $\bar{x}_i \in \mathbb{R}^d$. Let 
\[ c_{av} = \frac{1}{|C|} \sum\limits_{\bar{x}_i \in C} \bar{x}_i \] 
Prove that for any $c \in \mathbb{R}^d$,
\[ \sum\limits_{\bar{x}_i \in C} \left \lVert \bar{x}_i - c \right \rVert_2^2 \geq \sum\limits_{\bar{x}_i \in C} \left \lVert \bar{x}_i - c_{av} \right \rVert_2^2 \]

(Hint: $\bar{x}_i - c = \bar{x}_i - c_{av} + c_{av} -c$)
\end{problem} 

\begin{problem}{4. (30 points)}
Please see attached jupyter notebook.
\end{problem}
\end{document}

