{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Classification\n",
    "\n",
    "In this assignment, we will use logistic regression to judge the quality of wines. The dataset is taken from UCI machine learning repository. For description of the dataset, see [here](https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
    "\n",
    "Attributes of the dataset are listed as following:\n",
    "1. fixed acidity \n",
    "2. volatile acidity \n",
    "3. citric acid \n",
    "4. residual sugar \n",
    "5. chlorides \n",
    "6. free sulfur dioxide \n",
    "7. total sulfur dioxide \n",
    "8. density \n",
    "9. pH \n",
    "10. sulphates \n",
    "11. alcohol \n",
    "\n",
    "Output variable (based on sensory data): \n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "##### In the first part, we call the sklearn library for logistic regression. We have filled out most of the code for this part. In the second part, you will design your own Logistic Regression from scratch, Yayy! This will mean implementing the various building blocks we learnt in class. Also, keep in mind that this part will serve as a stepping stone later when we design our own neural network.\n",
    "\n",
    "\n",
    "The following code loads the dataset, and the dataset looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.114</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99647</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.086</td>\n",
       "      <td>15.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.99502</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>14.3</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.00080</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>8.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.084</td>\n",
       "      <td>13.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.99538</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.070</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.57</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.080</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99652</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.57</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.99586</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.59</td>\n",
       "      <td>12.1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.086</td>\n",
       "      <td>21.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>10.4</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.69</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.080</td>\n",
       "      <td>33.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "1518            7.4             0.470         0.46             2.2      0.114   \n",
       "1246            7.4             0.740         0.07             1.7      0.086   \n",
       "544            14.3             0.310         0.74             1.8      0.075   \n",
       "1343            7.5             0.510         0.02             1.7      0.084   \n",
       "428             9.1             0.520         0.33             1.3      0.070   \n",
       "1190            9.1             0.400         0.57             4.6      0.080   \n",
       "1056            8.9             0.480         0.53             4.0      0.101   \n",
       "361             8.6             0.450         0.31             2.6      0.086   \n",
       "367            10.4             0.575         0.61             2.6      0.076   \n",
       "1401            7.9             0.690         0.21             2.1      0.080   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "1518                  7.0                  20.0  0.99647  3.32       0.63   \n",
       "1246                 15.0                  48.0  0.99502  3.12       0.48   \n",
       "544                   6.0                  15.0  1.00080  2.86       0.79   \n",
       "1343                 13.0                  31.0  0.99538  3.36       0.54   \n",
       "428                   9.0                  30.0  0.99780  3.24       0.60   \n",
       "1190                  6.0                  20.0  0.99652  3.28       0.57   \n",
       "1056                  3.0                  10.0  0.99586  3.21       0.59   \n",
       "361                  21.0                  50.0  0.99820  3.37       0.91   \n",
       "367                  11.0                  24.0  1.00000  3.16       0.69   \n",
       "1401                 33.0                 141.0  0.99620  3.25       0.51   \n",
       "\n",
       "      alcohol  quality  \n",
       "1518     10.5        5  \n",
       "1246     10.0        5  \n",
       "544       8.4        6  \n",
       "1343     10.5        6  \n",
       "428       9.3        5  \n",
       "1190     12.5        6  \n",
       "1056     12.1        7  \n",
       "361       9.9        6  \n",
       "367       9.0        5  \n",
       "1401      9.9        5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "#train = np.genfromtxt('wine_training1.txt', delimiter=',')\n",
    "red = pd.read_csv('winequality-red.csv')\n",
    "white = pd.read_csv('winequality-white.csv')\n",
    "red = shuffle(red, random_state = 10)\n",
    "white = shuffle(white, random_state = 10)\n",
    "red.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "To convert this into a binary classification task, we split the quality into a binary feature *good* or *bad* depending on whether the quality is larger than 6 or not.\n",
    "\n",
    "Next for both red and white wines, we randomly pick $70\\%$ of the data to be our training set and the remaining for testing ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4731     True\n",
       "937     False\n",
       "1217    False\n",
       "3296    False\n",
       "4524     True\n",
       "3640     True\n",
       "785      True\n",
       "393      True\n",
       "562      True\n",
       "1285     True\n",
       "Name: quality, dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_red = red.iloc[:, :-1]\n",
    "y_red = red.iloc[:, -1] >= 6\n",
    "\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.3, random_state = 0)\n",
    "\n",
    "X_white = white.iloc[:, :-1]\n",
    "y_white = white.iloc[:, -1] >= 6\n",
    "X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.3, random_state = 0)\n",
    "\n",
    "#y_red.head(10)\n",
    "y_white.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3428, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_white.shape\n",
    "#y_train_white.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Logistic Regression for Red Wine and White Wine\n",
    "\n",
    "The code below trains Logistic Regression classifiers for red wine and white wine separately and tests them on the corresponding dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for red wine is: 0.275.\n"
     ]
    }
   ],
   "source": [
    "# Trained and tested on red wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import hamming_loss\n",
    "clf_red = LogisticRegression(solver = 'sag', max_iter=10000)\n",
    "clf_red.fit(X_train_red, y_train_red)\n",
    "y_pred_red = clf_red.predict(X_test_red)\n",
    "error_red = hamming_loss(y_test_red, y_pred_red)\n",
    "print('The testing error for red wine is: ' + str(error_red) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for white wine is: 0.2612244897959184.\n"
     ]
    }
   ],
   "source": [
    "# Trained and tested on white wine\n",
    "clf_white = LogisticRegression(solver = 'sag', max_iter=10000)\n",
    "clf_white.fit(X_train_white, y_train_white)\n",
    "y_pred_white = clf_white.predict(X_test_white)\n",
    "error_white = hamming_loss(y_test_white, y_pred_white)\n",
    "print('The testing error for white wine is: ' + str(error_white) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "What happens if you apply them to different domains? Test the model trained using 'X_trn_white, y_trn_white' on 'X_test_red', and vice versa. Print out the errors and compare with previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing error for red wine when using white wine training data model is: 0.35625.\n",
      "The testing error for white wine when using red wine training data model is: 0.3401360544217687.\n"
     ]
    }
   ],
   "source": [
    "#===== Your code here ======\n",
    "y_pred_red = clf_white.predict(X_test_red)\n",
    "error_red = hamming_loss(y_test_red, y_pred_red)\n",
    "\n",
    "y_pred_white = clf_red.predict(X_test_white)\n",
    "error_white = hamming_loss(y_test_white, y_pred_white)\n",
    "\n",
    "print('The testing error for red wine when using white wine training data model is: ' + str(error_red) + '.')\n",
    "print('The testing error for white wine when using red wine training data model is: ' + str(error_white) + '.')\n",
    "#===== Your code here ======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.2 Do the test errors increase or decrease? Explain why this happens.\n",
    " ### This is because the logistic regression classifier that is used to classify the testing data of each type of wine did not come from the training result of the same type of wine. When a logistic regression classifier is being trained with a set of training data, the goal is to mold the classifier to be able to classify or predict the labels for a similar type of data. So for example, when the logistic regression is being trained with the data of red wine, then the classifier is meant to classify or predict the labels of other red wine, given the values of the features. In this case, when the data that is given to predict is of white wine, then the logistic regression will not be able to classify well, since red and white wine will have different tendencies/behavior throughout the features. Thus, when you apply test data to different domains, the error will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 The effect of regularization\n",
    "In this section we will investigate the effect of regularization. The code below runs logistic regression in sklearn, using $\\ell_2$ regularization with regularizer value $\\lambda$ in the set $\\{0.00001 \\times 4^i: i = 0,1,2,..., 15\\}$. (The regularization parameter 'C' in scikit-learn is the inverse of $\\lambda$ we see in class). Training and test errors are plotted with respect to the regularizer value $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11e75f190>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnQxZJgEAWtrCEsIYdU3BBFnGtWnCruFfttfTWpdpFq7fLvfbeq73+6lJtKbVeeyuKilKp+8ImiEBQBAMJO0kEskFCEsg6n98fM0CEQGaSOZnJzOf5ePBIZs72mUN458v3nPP9iqpijDEmfEUFuwBjjDHOsqA3xpgwZ0FvjDFhzoLeGGPCnAW9McaEOQt6Y4wJc52CXUBzkpOTdeDAgcEuwxhjOoz169eXqmpKc8tCMugHDhxIdnZ2sMswxpgOQ0T2nGqZdd0YY0yYs6A3xpgwZ0FvjDFhLiT76JtTX19PYWEhNTU1wS7FcXFxcaSlpREdHR3sUowxYaDDBH1hYSFdunRh4MCBiEiwy3GMqlJWVkZhYSHp6enBLscYEwY6TNdNTU0NSUlJYR3yACJCUlJSRPzPxRjTPjpM0ANhH/JHRcrnNMY0UV4AO5Y6susO03UTTGVlZcyYMQOA/fv343K5SEnxPJewdu1aYmJiTrv9smXLiImJ4ZxzznG8VmNMB3TkILx4NRwuhXs3QmxCQHdvQe+DpKQkNmzYAMBvfvMbEhIS+OlPf+rz9suWLSMhIcGC3hhzsvoaWHAjHNwFN70R8JCHDtZ1E0rWr1/P1KlTOfPMM7n44ovZt28fAE8//TSZmZmMGTOG2bNns3v3bubOncsTTzzBuHHj+OSTT4JcuTEmZLjd8I85sGcVzPoTpJ/nyGE6ZIv+3/+Zw+a9hwK6z8w+Xfn1FSN9WldVufvuu3nzzTdJSUnhlVde4eGHH+b555/n0UcfZdeuXcTGxlJeXk5iYiJz5szx+38BxpgI8MG/Qc4iuOi3MPoaxw7TIYM+2Gpra/nqq6+48MILAWhsbKR3794AjBkzhhtvvJFZs2Yxa9asYJZpjAllq5+Fz56FST+Es+9y9FAdMuh9bXk7RVUZOXIkq1evPmnZ22+/zYoVK1i8eDGPPPIIOTk5QajQGBPSvnoD3n8IMmfCxf8FDt9pZ330rRAbG0tJScmxoK+vrycnJwe3201BQQHTp0/nd7/7HeXl5VRVVdGlSxcqKyuDXLUxJlAa3dr6jXevhEU/gP7nwJXzIMr5GO6QLfpgi4qKYuHChdxzzz1UVFTQ0NDAj3/8Y4YOHcpNN91ERUUFqsp9991HYmIiV1xxBddccw1vvvkmf/jDHzjvPGcuuBhjAqeh0U3BwSPsKK5iZ2kVO4qrPV9Lqjlc18DDl2Vy06T+/j33UrwFFtwA3dNh9nyIjnPuAzQhqm34zeSQrKwsPXE8+i1btjBixIggVdT+Iu3zGhMsFUfq2VniCfAdJVXHvt9TVk194/F8TE6IYVBKAhkp8RQcOMLK7aVcNb4v/3nlaM6IcbV8oEN74bkLwN0I3/8QEvsH9HOIyHpVzWpumbXojTFhr9GtfH3wCDtKqrx/joZ6NaVVtcfW6xQlDEjqTEZKAheM6ElGSjwZqQlkJCfQrfPxQQbdbuWZpdt54qOtbN53iD/ddCbpyfGnLqCmAl68BmoOwW3vBDzkW2JBb4wJSw2Nbn779hZW7yhjV1k1dQ3uY8sSO0czOCWB84enkJGScKyl3q9HZ6JdLfeZR0UJ98wYwth+idy74Au+84eV/M+1Y7lkVK9mCqmDV26C0jy48TXoPSaQH9MnFvTGmLD0cW4xL3y6m3MHJzF1WIqnde4N9R7xpx+2xFdTh6bw1t2T+dH8z5nz4np+MGUQP7t4GJ2O/rJwu+HNf4VdK+DKP0PG+QE5rr98CnoRuQR4CnABz6nqoycsnwk8AriBBuDHqrrSuywReA4YBShwu6qefF+iMcYE0Etr8unVNY6/3TbxePA6IK17Z16dczaPvLWZP6/YyRcF5Txzw3hSu8TBx/8Om16DGb+CsbMdq6ElLX56EXEBzwKXApnA9SKSecJqHwNjVXUccDueYD/qKeA9VR0OjAW2BKJwY4w5lYIDh1mxrYTrvtXP0ZA/KraTi9/OGs3vvzuWjYXlXPb0Sna/+ySsehKy7oDJ9ztew+n4cgYmAttVdaeq1gELgJlNV1DVKj1++048npY7ItIVmAL81btenaqWB6p4Y4xpzstr8xFg9sR+7Xrcqyak8Y8fncslrnX0/+w37E6ehl76O8cfiGqJL0HfFyho8rrQ+943iMiVIpILvI2nVQ8wCCgB/ldEvhCR50Sk2UvTInKniGSLSHZJSYlfH8JpZWVljBs3jnHjxtGrVy/69u177HVdXd1pt83Ozuaee+5pp0qNMXUNbl7NLuT84T3p3e2Mdj/+8Lot/EfDk+yOG84lhbfyowVfUllT3+51NOVL0Df3q+ikm+9VdZG3e2YWnv568FwDmAD8SVXHA9XAg80dRFXnqWqWqmYdHes9VBwdpnjDhg3MmTOH++6779jrmJgYGhoaTrltVlYWTz/9dDtWa0xk+3BzEaVVtdw4qX1vYQSgdBu8fB3StS/p97zF/d8ey/s5Rcx8dhVbi4L3dLwvQV8INP3/Txqw91Qrq+oKIENEkr3bFqrqGu/ihXiCv8P73ve+x/3338/06dN54IEHWLt2Leeccw7jx4/nnHPOIS8vD/CMRX/55ZcDnrHsb7/9dqZNm8agQYPsF4AxDnhp7R76Jp7BlKHt3GCsLIIXr4KoTnDT60h8MndOyWD+9ydx6EgDM59ZxZsbvm7fmrx8uetmHTBERNKBr4HZwA1NVxCRwcAOVVURmQDEAGXe1wUiMkxV84AZwOY2V/3ug7B/U5t38w29RsOlj7a8XhNbt27lo48+wuVycejQIVasWEGnTp346KOPeOihh3j99ddP2iY3N5elS5dSWVnJsGHD+OEPf0h0dHQzezfG+GtXaTWrtpfx04uG4opqx37x2kqYfw1Ul8H33oIe6ccWnTUoiXfumcxdL33BvQs28Pmegzx8WSYxndpvqLEWg15VG0TkLuB9PLdXPq+qOSIyx7t8LnA1cIuI1ANHgOuaXJy9G5gvIjHATuA2Bz5HUFx77bW4XJ5HnysqKrj11lvZtm0bIkJ9ffN9cpdddhmxsbHExsaSmppKUVERaWlp7Vm2MWHr5bX5dIoSvpvVjhdhG+vh1VuhKAdueAX6ntxpkdo1jvn/Mon/eT+PeSt28mVhBX+8cQJ9EtvnGoJP99Gr6jvAOye8N7fJ948Bj51i2w1As+MvtJqfLW+nxMcfv678y1/+kunTp7No0SJ2797NtGnTmt0mNjb22Pcul+u0/fvGGN/VNjTyWnYBF2b2JLVr+wwWhiosvgd2fAzfeQaGXHjKVaNdUTz07RGM75fIzxZu5PI/rOTp2eOZPCTZ8TJtmOIAqaiooG9fz81IL7zwQnCLMSYCvffVfg4erueG9rwIu/Q/4cuXYNovYMLNPm1y6ejeLL7rXFISYrn5+TU8s2Qb7rYMe+wDC/oA+fnPf84vfvELzj33XBobG4NdjjERZ/6afAYkdebcDOdbyABk/y+s+B8YfzNMfcCvTQelJLDoR+cwc2wfHv9gK9//v2wqDjt3C6YNUxyiIu3zGtMW24oqufCJFTx46XDmTM1w/oB578GC6yFjBlz/Mrhad0OFqvLimnz+45859Owax9ybzmRU326t2tfphim2Fr0xpsN7aW0+0S7hmjPb4caGos2w8DboNQaufaHVIQ8gItx81gBe/cHZNLqVm/+6hurawF+3s9ErjTEdWk19I6+vL+SSUb1JTohteYO2+ujX4IrxDDkcmxCQXY7v35237p7Mln2VxMcGPpatRW+M6dDe2riPQzUN3DCxHS7C5q+BbR/AufdCQmpAd52UEOvYHTgdKuhD8XqCEyLlcxoTCC+t2cOglHjOGtTD2QOpwpJHID4VJv3A2WMFWIcJ+ri4OMrKysI+BFWVsrIy4uLa6T5gYzqwLfsO8Xl+OTdM9HOS7tbYuQx2fwLn/QRiTjNtYAjqMH30aWlpFBYWEmojWzohLi7OnpY1xgcvrcknplOU8xdhj7bmu6ZBVsd7uL/DBH10dDTp6ektr2iMiQjVtQ0s+uJrLh/dm8TOgZka8JTy3oWv18MVT0OndrjgG2AdpuvGGGOa+ueXe6mqbXD+SVi32/MEbI9BMO6GltcPQR2mRW+MMU29tDafYT27cOaA7s4eKOcNKPoKrnquTffMB5O16I0xHc6mwgo2FlZwwySHL8I2NsCy/4bUTBh1tXPHcZi16I0xHc5La/dwRrSLKyecNKtpYH35MpRth+vmQ1THbRd33MqNMRGpsqaeNzfs5Yqxveka52BXSkMtLH8M+kyA4Zc5d5x2YC16Y0yH8o8Nezlc18gNkwY4e6D1f4OKArjiKXD6Hn2HWYveGNNhqCovrclnZJ+ujE1r3SiPPqk7DJ88DgPOhYzznTtOO7GgN8Z0GF8UlLNl3yHnL8KunQdVRXD+Lzt8ax4s6I0xHchLa/KJj3Exc5yDF2FrKmDVkzD4AhhwtnPHaUcW9MaYDqHicD1vbdzLzPF9SXBgKN9jVv8RjhyE8//NuWO0Mwt6Y0yH8MYXhdTUu50djvjwAVj9LIy4AvqMd+447cyC3hgT8o5ehB3bL7HVU+35ZOUTUFcF0x927hhBYEFvjAl563YfZFtxFTc62Zqv3A9r/wJjvgup4TVfs09BLyKXiEieiGwXkQebWT5TRDaKyAYRyRaRyScsd4nIFyLyVqAKN8ZEjpfW7KFLXCcuH9vbuYOseBzc9TDtpIjr8FoMehFxAc8ClwKZwPUiknnCah8DY1V1HHA78NwJy+8FtrS9XGNMpDlQXcc7X+3nqvF96Rzj0EXYg3tg/Qsw/ibPKJVhxpcW/URgu6ruVNU6YAEws+kKqlqlx6d+igeOTQMlImnAZZwc/sYY06LX1xdS1+B29knY5Y+BRMGUnzt3jCDyJej7AgVNXhd63/sGEblSRHKBt/G06o96Evg54D7dQUTkTm+3T3YkzCJljGmZqvLS2nyyBnRnWK8uzhykZKtn8LJv3QHdHB4kLUh8CfrmHgs7aeJWVV2kqsOBWcAjACJyOVCsqutbOoiqzlPVLFXNSklJ8aEsY0y4W72jjF2l1c5OLrLsv6DTGTD5fueOEWS+BH0h0K/J6zRg76lWVtUVQIaIJAPnAt8Rkd14unzOF5EXW1+uMSaSzF+bT2LnaL492qGLsPs2Qs4iOGsOJIRvA9OXoF8HDBGRdBGJAWYDi5uuICKDxTvwhIhMAGKAMlX9haqmqepA73ZLVPWmgH4CY0xYKqms5YOc/Vw9IY24aJczB1n6nxDbDc6525n9h4gWL2GraoOI3AW8D7iA51U1R0TmeJfPBa4GbhGReuAIcF2Ti7PGGOO319YXUN+oXO/UvfMF62Dre56hDs5weDrCIPPpXiVVfQd454T35jb5/jHgsRb2sQxY5neFxpiI43YrC9YWcNagHgxOTXDmIEv+Azonw6QfOrP/EGJPxhpjQs7K7aXkHzjs3C2VO5fDrhVw3v0Q69AvkhBiQW+MCTnz1+whKT6Gi0f2DPzOVWHJI9C1L2TdEfj9hyALemNMSCk6VMNHW4q5JiuN2E4OXITd+j4UroMpP4PouMDvPwRZ0BtjQsor6wpodKszwxG73bDkt9A93TPcQYSwycGNMSGj0a0sWJvPeUOSGZAUH/gDbP4HFG2CK+eBKzrw+w9R1qI3xoSM5VuL2VtR40xrvrEBlv4XpAyH0dcEfv8hzFr0xpiQMf+zfFK6xHJBpgMXYTe+AmXb4Lt/hyiHHsAKUdaiN8aEhK/Lj7A0r5jrsvoR7QpwNDXUwfJHofc4zzSBEcZa9MaYkPDK2nwUmD2xX4vr+u3zv0F5Plz2BEhz4zSGN2vRG2OCrqHRzSvZBUwbmkJa986B3XndYc/sUf3PhsEzArvvDsKC3hgTdB/nFlN0qNaZJ2HXPQdV++H8X0Zkax4s6I0xIeDltfn07hbH9GEBHiq45hCsfAIyzoeB5wZ23x2IBb0xJqgqa+pZua2U74ztQ6dAX4T97E9w5IBnhMoIZkFvjAmqldtKaXAr5w9PDeyODx+A1c/A8Muh75mB3XcHY0FvjAmqJbnFdI3rxJkDAjwm/Gd/hNpDMO0Xgd1vB2RBb4wJGrdbWZpXzJShKYHttjl8AD6bC5kzodeowO23g7KgN8YEzaavKyitqmPGiAB326x+FuoqYeqDgd1vB2VBb4wJmiW5xYjA1KEBDPrDB2DNXMicBT0zA7ffDsyC3hgTNEvzihnfL5Ee8TGB2+nqZ6CuGqY+ELh9dnAW9MaYoCiurGFjYUVg77apLoM1f4aR1ppvyoLeGBMUy3JLAJgeyKC31nyzLOiNMUGxJLeYXl3jyOzdNTA7rC6DtfNg5JWQOiIw+wwTFvTGmHZX1+Bm5fZSpg9PRQI1/szqP1hr/hR8CnoRuURE8kRku4icdL+SiMwUkY0iskFEskVksvf9fiKyVES2iEiOiNwb6A9gjOl41u0+QFVtQ+D656tLYc08GHU1pA4PzD7DSIvj0YuIC3gWuBAoBNaJyGJV3dxktY+BxaqqIjIGeBUYDjQAP1HVz0WkC7BeRD48YVtjTIRZkltMTKcozh2cFJgdfvo01B+GqT8PzP7CjC8t+onAdlXdqap1wAJgZtMVVLVKVdX7Mh5Q7/v7VPVz7/eVwBagb6CKN8Z0TEtyizlrUBKdYwIw91F1Kaz9i2ce2JRhbd9fGPIl6PsCBU1eF9JMWIvIlSKSC7wN3N7M8oHAeGBNawo1xoSHXaXV7CqtZkagum1WPQUNNTDFWvOn4kvQN3elRE96Q3WRqg4HZgGPfGMHIgnA68CPVfVQswcRudPbv59dUlLiQ1nGmI5oSW4xQGD656tKPBOLjLoGUoa2fX9hypegLwSaTuKYBuw91cqqugLIEJFkABGJxhPy81X1jdNsN09Vs1Q1KyUlwJMPGGNCxtLcYganJtCvRwCmDPzU25q3vvnT8iXo1wFDRCRdRGKA2cDipiuIyGDx3iMlIhOAGKDM+95fgS2q+vvAlm6M6WiqahtYs6sscK35tc/B6GsheUjb9xfGWrwSoqoNInIX8D7gAp5X1RwRmeNdPhe4GrhFROqBI8B13jtwJgM3A5tEZIN3lw+p6jtOfBhjTGhbua2E+sYATTKy6klorLW+eR/4dMnbG8zvnPDe3CbfPwY81sx2K2m+j98YE4GW5BbTJRCTjFQVw7q/wujvQvLgwBQXxuzJWGNMu/BMMlLClKEpRLd1kpFVT3la89Y37xMLemNMu8jZe4iSylrOH9bGbpvKIk9rfsx1kJQRmOLCnAW9MaZdHJ1kZNqwNt5Vt+opaKyDKT8LTGERwILeGNMuluQWMa5fIkkJsa3fSeV+yLbWvL8s6I0xjiuprOXLwoq2d9usegoa62HKTwNTWISwoDfGOG5Znudp2DZNMlK5H7Kfh7GzrTXvJwt6Y4zjluYV07NrLCP7tGGSkZVPWmu+lSzojTGOqmtw88nWUqYPa8MkI4f2eVrz466HHoMCW2AEsKA3xjgqe/cBKts6ycjKJ0Ab4TxrzbeGBb0xxlFLcouJcUVx7uDk1u3g0F5Y/wKMvR56pAe0tkhhQW+McdSSvGImDepBfGwrJxk52pq3vvlWs6A3xjhmd2k1O0uqW99tc7Q1P+4G6D4wkKVFFAt6Y4xj2jzJyCe/B3Vb33wbWdAbYxyzNK+YjJR4BiTF+79xxdfw+d9g3I3QfUDgi4sgFvTGGEdU1zawZueB1rfmV3pb89Y332YW9MYYR6zcXkpdo7t1T8NWFMLn/wfjb4LE/oEvLsJY0BtjHLE0t5gusZ341sAe/m/8ye9BFc77SeALi0AW9MaYgFNVluQWt26SkfICa80HmAW9MSbgcvYeoriytnXdNit/7/lqrfmAsaA3xgRcqycZKc+Hz/8OE26GxH7OFBeBLOiNMQG3JLeYMWmJJPs7ycgnvwcRa80HmAW9MSagSqtq+bKw3P9JRsrz4YsXYcIt0C3NmeIilAW9MSagluWVoAozRvgZ9Cse97TmJ9/vTGERzILeGBNQS3OLSe3i5yQjB/fAhvkw4Vbo1te54iKUT0EvIpeISJ6IbBeRB5tZPlNENorIBhHJFpHJvm5rjAkf9Y1uVmwt8X+SkU/+H0gUTL7PueIiWItBLyIu4FngUiATuF5EMk9Y7WNgrKqOA24HnvNjW2NMmMjefZDK2gb/bqusrYKNr3rmgrXWvCN8adFPBLar6k5VrQMWADObrqCqVaqq3pfxgPq6rTEmfCzNKybaJUwe4sckI3nvQMMRz8QixhG+BH1foKDJ60Lve98gIleKSC7wNp5Wvc/bere/09vtk11SUuJL7caYEPPxliLOGpREgj+TjGxaCF37Qr+znCsswvkS9M11tOlJb6guUtXhwCzgEX+29W4/T1WzVDUrJcXPhyyMMUGXX3aYHSXVTPfntsrqMtjxMYy6GqLs3hCn+HJmC4Gmj6ilAXtPtbKqrgAyRCTZ322NMR3XktwiwM9JRjb/A9wNMPpah6oy4FvQrwOGiEi6iMQAs4HFTVcQkcHivcQuIhOAGKDMl22NMeFhSV4Jg5LjGZjsxyQjX70OyUOh12jnCjO02JGmqg0ichfwPuACnlfVHBGZ410+F7gauEVE6oEjwHXei7PNbuvQZzHGBEl1bQOf7SjjlrP9mAmqohD2rILpD3selDKO8emKiaq+A7xzwntzm3z/GPCYr9saY8LLKu8kI35123z1uufrqKudKcocY1c/jDFttjSvmITYTmT5M8nIpoXQZwIkZThXmAEs6I0xbaSqLM0t4bwhycR08jFSSvJg/0a7CNtOLOiNMW2yed8h9h+q8e9p2E0LAYFRVzlWlznOgt4Y0yZLthQD+H7/vCpseg3Sp0CXXg5WZo6yoDfGtMmSvGLGpnUjpYuPk4zs/RwO7oLR1zhbmDnGgt4Y02plVbVsKCj3v9vGFQMjrnCuMPMNFvTGmFZbvtUzyYjPt1W6Gz23VQ65CM7o7mxx5hgLemNMqy3JLSY5IZZRfbr5tsHulVBVZPfOtzMLemNMq9Q3ulm+tYTzh6cQFeXjk62bXoOYBBh6ibPFmW+woDfGtMr6PQeprGnwvdumoRY2L4bhl0NMZ2eLM99gQW+MaZWluUcnGfFxWPHtH0FthT0kFQQW9MaYVlmSW8zE9B6+TzKy6TXonASDpjpbmDmJBb0xxm8FBw6zrbjK94ekaish710YeSW4op0tzpzEgt4Y47cluZ6nYWeM6OnbBrlvQ0ONddsEiQW9McZvS3KLSU+OJ93XSUY2LYRu/SBtorOFmWZZ0Btj/HK4roHVO8t877apLoUdS2xe2CCys26M8cun28uoa/BjkpGcRaCN1m0TRBb0xhi/LMkrJj7GxcR0HycZ+ep1SBkBPUc6W5g5JQt6Y4zPPJOMFHPekBTfJhkpz4f81TD6apsXNogs6I0xPtuyr5J9FTW+d9scmxfWhiQOJh+fdDDGRJr6RjcFBw6zo6SanSVV7Cip4ov8cgCmDffxadhNr0Pat6BHuoOVmpZY0BsT4SoO17O9pMob5sdDfU/ZYRrcemy95IRYBqXE87OLh5HaJa7lHRdvgaJNcMljDlZvfGFBb0wEaHQrhQcPs6Okih3F1ewsPf61tKru2HrRLmFAUjyDUxO4aGQvMlISGJQST0ZyAt06+/lE66aFIFGep2FNUPkU9CJyCfAU4AKeU9VHT1h+I/CA92UV8ENV/dK77D7g+4ACm4DbVLUmMOUbY5qzq7SaRZ8XsrWoip2lVewuPUxdo/vY8h7xMQxKjmfG8J5kpMYzKDmBjNQE+nU/g06uAFy6U4WvFkL6VOji49OzxjEtBr2IuIBngQuBQmCdiCxW1c1NVtsFTFXVgyJyKTAPmCQifYF7gExVPSIirwKzgRcC/DmMMcCXBeXMXb6D93L2EyXCgB6dGZSSwPRhqcda54NSEugRH+NsIV+vh4O7YcrPnT2O8YkvLfqJwHZV3QkgIguAmcCxoFfVT5us/xmQdsIxzhCReqAzsLetRRtjjlNVlm8tYe7yHXy28wBd4zrxr9MyuPWcgb71pTth02vgioURlwfn+OYbfAn6vkBBk9eFwKTTrH8H8C6Aqn4tIo8D+cAR4ANV/aC5jUTkTuBOgP79+/tQljGRraHRzdub9jF3+U627DtEr65xPPztEVw/qb/vQwc7obEBvnoDhl4EcT5OMWgc5ctPQ3NPOWgz7yEi0/EE/WTv6+54Wv/pQDnwmojcpKovnrRD1Xl4unzIyspqdv/GGDhS18ir2QX85ZOdFB48QkZKPL+7ZgyzxvX17SEmp+3+BKqLbciDEOJL0BcC/Zq8TqOZ7hcRGQM8B1yqqmXety8AdqlqiXedN4BzgJOC3hhzeger6/jb6t387dPdHDxcz5kDuvPrK0YyY3iq73O2todNCyGmCwy5KNiVGC9fgn4dMERE0oGv8VxMvaHpCiLSH3gDuFlVtzZZlA+cJSKd8XTdzACyA1G4MZGi8OBhnvtkF6+sK+BIfSMzhqcyZ1oG3xro41gz7am+BrYshhFXQPQZwa7GeLUY9KraICJ3Ae/jub3yeVXNEZE53uVzgV8BScAfxTOeRYOqZqnqGhFZCHwONABf4O2eMcac3pZ9h/jz8h38c+M+BJg5ri93ThnEsF5dgl3aqW3/EGoPwWgb8iCUiGrodYdnZWVpdrY1/E3kUVXW7DrA3OU7WJZXQucYF9dP7M8dk9Ppk9gBWsiv3gJ7PoX7c8Flz2O2JxFZr6pZzS2zvwljQoDbrXywuYi5y3ewoaCcpPgYfnLhUG4+ewCJnR2+5z1Qag5B3ntw5q0W8iHG/jaMCaKyqlrey9nPX1fuYmdJNf17dOaRWaO49sw04qJdwS7PP7lvQWOt3W0TgizojWlnu0qr+XDzfj7cXMT6PQdxK4zs05U/XD+eS0f1CswQBCWJ8BwAAAqjSURBVMGwaSEk9veMVmlCigW9MQ5zu5UvC8v5cHMRH24uYltxFQAjenflrvOHcFFmT0b26Yp05Ik5qoph5zI4916bYCQEhVXQH65rCHYJJkREu6KIDmLLuLahkU93lPFBThEfbymiuLIWV5QwKb0HN0zqzwUjetKvR+eg1RdwOf+weWFDWFgF/ZmPfMSR+sZgl2FCQJRAvx6dPQN5JceTkXr8a1J8jCOt54rD9SzJ87Tal+eVUF3XSHyMi6nDUrgwsyfTh6V2nAur/vpqIaSOhJ6Zwa7ENCOsgv6nFw+joclQrCZyVdU2sLO0mh3FVazaXkptw/Gfi25nRHvGWD861npKAhkpCfTv0dnvIQQKDx4+1iWzZtcBGt1KSpdYvjOuLxdl9uTsjKSOd1HVXwd3Q8EamPGrYFdiTiGsgv6OyTZdmTmZ2618XX7kWPDvKKliZ0k1K7aWsHB94bH1XFFHh/U9+ZdAd++wvqpKzt5DfOAN9y37DgEwJDWBH0wZxIWZPRmblhhaQxI47di8sFcHtw5zSmEV9MY0JypK6NejM/16dGbq0G/OdVpZU8/OkuMzLh3/JVD6jYk6uneOZlBKAvsravi6/AgikDWgOw99ezgXZvYiPTm+vT9W6Nj0OvSbBN0HBrsScwoW9CaidYmLZmy/RMb2S/zG+41u5euDRzxT73nnUt1RUkVmn67ce8EQZgxPJSkhNkhVh5CiHCjOgW8/HuxKzGlY0BvTDFeU0D+pM/2TOjN9eGqwywldmxaCuCBzVrArMafRQZ/MMMYE3dF5YQdNg4SUltY2QWRBb4xpncJ1UJ5v9853ABb0xpjW2fQadIqD4ZcFuxLTAgt6Y4z/GhsgZxEMvRjiuga7GtMCC3pjjP92LYfqEuu26SAs6I0x/tu0EGK7weALg12J8YEFvTHGP/VHYMs/vfPCxgW7GuMDC3pjjH+2fQB1lTYvbAdiQW+M8c+m1yA+FdKnBLsS4yMLemOM746Uw9YPYNRVEBXmo3KGEQt6Y4zvchbZvLAdkAW9McY3BWvh/Yeg9zjoe2awqzF+8CnoReQSEckTke0i8mAzy28UkY3eP5+KyNgmyxJFZKGI5IrIFhE5O5AfwBjTDvZvgvnXQJdecMMrNi9sB9Pi6JUi4gKeBS4ECoF1IrJYVTc3WW0XMFVVD4rIpcA8YJJ32VPAe6p6jYjEAGE0UaYxEaB0O/z9SohJgFve9IS96VB8adFPBLar6k5VrQMWADObrqCqn6rqQe/Lz4A0ABHpCkwB/updr05VywNVvDHGYeUF8H8zPSNV3vImJPYPdkWmFXwJ+r5AQZPXhd73TuUO4F3v94OAEuB/ReQLEXlORCJ4Kh5jOpCqYk/I11bCzYsgeUiwKzKt5EvQN9cZp82uKDIdT9A/4H2rEzAB+JOqjgeqgZP6+L3b3iki2SKSXVJS4kNZxhjHHDno6a6p3Ac3vga9xwS7ItMGvgR9IdCvyes0YO+JK4nIGOA5YKaqljXZtlBV13hfL8QT/CdR1XmqmqWqWSkpNomBMUFTWwXzr4XSrTB7PvSf1PI2JqT5EvTrgCEiku69mDobWNx0BRHpD7wB3KyqW4++r6r7gQIRGeZ9awbQ9CKuMSaU1NfAguvh68/hmuch4/xgV2QCoMW7blS1QUTuAt4HXMDzqpojInO8y+cCvwKSgD+K57arBlXN8u7ibmC+95fETuC2wH8MY0ybNdbDwttg1wqYNdczaJkJC6LabHd7UGVlZWl2dnawyzAmcrjdsOgHsOlV+PbjMPFfgl2R8ZOIrG/SwP4GezLWmEinCu/8xBPy5//SQj4MWdAbE8lU4aNfQ/bzcO6P4byfBLsi4wALemMi2Sf/D1Y9BVl3wAW/saENwpQFvTGRas08WPIIjP6up1/eQj5sWdAbE4k2vAzv/gyGXQaz/ghRFgXhzP52jYk0W/4Jb/4rpE/13Cvvig52RcZhFvTGRJIdS2Dh7Z7x5Ge/ZJN7RwgLemMiRf5nsOBGSB7qGb8mNiHYFZl2YkFvTCTY9yXM/y506e0ZifKM7sGuyLQjC3pjwl3JVvj7VRDbxTOmfEJqsCsy7cyC3phwVp4Pf5/luXXyljchsV/L25iw0+KgZsaYAFKFAzth3wbYu8Hzdf9XniCOS4QzEv37Gtvl1Pe/VxZ5Jg6pq4LvvQ3Jg9v3s5qQYUFvjFPcbji4C/Z+0STYN0JthWe5KwZ6joQRl0NUJzhS7pnw48gBzy+DmnKoqQB1n/oY4oK4bs3/Iti9yhP2t7wJvUa3z2c2ISm8gr66rOV1TGSIckFs1/Z7EMjtbtJS/8Jz8XPfl1B7yLPcFesJ9dFXQ+9x0GccpIyATjEt77eu0vNLoKbct68Hd3u+RnWC61+Cft9y/OOb0BZeQf/kKKg/HOwqTMgQiOvachfIGd1Pfi+226l/SbjdcGDH8a6XvRs8oV5X6VnuioVeo2D0tZ5A7z0OUke07sGkqChPiz2uGzCg1WfCRLbwCvqLfgvuxmBXYUJBY52n2+PE1m7lvuOvG+tOs4NT/JKoLvV0vxwN9U5x0HMUjL2uSUt9uD1takJKeAX9t+4IdgWmo1CF+iO+d4fUlENxrif8x84+3lJPGQ6u8PpnZMKP/YSayCQCMZ09f7r2CXY1xjjK7qM3xpgwZ0FvjDFhzoLeGGPCnAW9McaEOQt6Y4wJcxb0xhgT5izojTEmzFnQG2NMmBNVDXYNJxGREmBPsOsIkGSgNNhFhDA7P6dn56dldo48BqhqSnMLQjLow4mIZKtqVrDrCFV2fk7Pzk/L7By1zLpujDEmzFnQG2NMmLOgd968YBcQ4uz8nJ6dn5bZOWqB9dEbY0yYsxa9McaEOQt6Y4wJcxb0xhgT5izog0xE4kVkvYhcHuxaQo2IzBKRv4jImyJyUbDrCQXen5e/ec/LjcGuJ9TYz0zzLOhbSUSeF5FiEfnqhPcvEZE8EdkuIg/6sKsHgFedqTJ4AnF+VPUfqvovwPeA6xwsN6j8PFdXAQu95+U77V5sEPhzfiLlZ8ZfFvSt9wJwSdM3RMQFPAtcCmQC14tIpoiMFpG3TviTKiIXAJuBovYuvh28QBvPT5NN/827Xbh6AR/PFZAGFHhXa2zHGoPpBXw/P0eF+8+MX2xy8FZS1RUiMvCEtycC21V1J4CILABmqup/Ayd1zYjIdCAezw/qERF5R1XdjhbeTgJ0fgR4FHhXVT93tuLg8edcAYV4wn4DEdJQ8+f8iMgWIuBnxl8W9IHVl+OtLfD8o5x0qpVV9WEAEfkeUBouIX8afp0f4G7gAqCbiAxW1blOFhdiTnWungaeEZHLgH8Go7AQcarzE8k/M6dkQR9Y0sx7LT6RpqovBL6UkOTX+VHVp/EEWyRq9lypajVwW3sXE4JOdX4i+WfmlCLiv37tqBDo1+R1GrA3SLWEIjs/vrNzdXp2fvxgQR9Y64AhIpIuIjHAbGBxkGsKJXZ+fGfn6vTs/PjBgr6VRORlYDUwTEQKReQOVW0A7gLeB7YAr6pqTjDrDBY7P76zc3V6dn7azgY1M8aYMGctemOMCXMW9MYYE+Ys6I0xJsxZ0BtjTJizoDfGmDBnQW+MMWHOgt4YY8KcBb0xxoQ5C3pjjAlz/x8OznNr0Z8dMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = np.array(range(0,15))\n",
    "lamb = 0.00001*(4**N)\n",
    "error_trn = np.zeros(15)\n",
    "error_tst = np.zeros(15)\n",
    "for i in N:\n",
    "    c = lamb[i]\n",
    "    clf = LogisticRegression( C = 1/c, solver = 'sag', max_iter= 10000)\n",
    "    clf.fit(X_train_red, y_train_red)\n",
    "    y_pred = clf.predict(X_test_red)\n",
    "    error_tst[i] = hamming_loss(y_test_red, y_pred)\n",
    "    y_pred_trn = clf.predict(X_train_red)\n",
    "    error_trn[i] = hamming_loss(y_train_red, y_pred_trn)\n",
    "plt.figure(1)\n",
    "plt.semilogx(lamb, error_tst, label = 'Test')\n",
    "plt.semilogx(lamb, error_trn, label = 'Train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the effect of regularization on training and test errors. Are they effected differently?\n",
    "### Yes, the effect of regularization on training and testing errors are different. Regularization is being used to prevent overfitting. In regularization, we are setting the parameter lambda to determine the penalty to regularize. When lambda is small (for example: lambda = 10^-5), we will have overfitting. Then, as we increase lambda to be bigger (lambda = 10^-1), we can see that overfitting is reduced and we are getting a more accurate prediction for our testing. However, the training error is not effected as they cannot decrease anymore (the slight dip on the plot is just noise and is negligible). This is how regularization effects differently, where testing prediction will improve but no improvement in training prediction. Then, as the lambda keeps on getting bigger (lambda = 10),it keeps on reducing complexity and the model became too simple, so training and testing error will just increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 Implementing Logistic Regression from Scratch\n",
    "In this section, we will implement a logistic regression classifier from scratch and train it using gradient descent. You are **NOT** allowed to use any pre-built logistic regression classifiers or gradient solvers from any machine learning libraries. \n",
    "\n",
    "Before we start implementing, it helps to know that when working with data in high dimensional space, **vectorization** is a technique that may significantly help decrease the running time of your code. Simply put, compared to using for-loops, it is usually preferred to directly work with matrices and vectors as a whole and compute expressions using built-in matrix/vector operations when no significant overhead in computation is introduced. \n",
    "\n",
    "Here is a simple example. Suppose we have two vectors $a\\in\\mathbb{R}^n$ and $x\\in\\mathbb{R}^n$, and we want to compute the following expression:\n",
    "$$\n",
    "c=\\sum_{i=1}^na_ix_i\n",
    "$$\n",
    "One could naively implement this using a for loop. However, note that $c=a\\cdot x$. If we use the dot product/matrix multiplication function in numpy, the running time can be significantly reduced, as you will observe in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized: 0.00090789794921875 seconds\n",
      "For loop: 0.6784160137176514 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N = 1000000\n",
    "a = np.random.normal(0, 1, N)\n",
    "x = np.random.normal(0, 1, N)\n",
    "\n",
    "# Vectorized code\n",
    "start_time = time.time()\n",
    "c = np.dot(a, x)\n",
    "print(\"Vectorized: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Dummy code\n",
    "start_time = time.time()\n",
    "c = 0\n",
    "for i in range(N):\n",
    "    c += a[i]*x[i]\n",
    "print(\"For loop: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason when we work with a dataset, it is preferred to process it as an entire batch (or several batches) using vectorization.\n",
    "Throughout this this problem, we define the following expressions:\n",
    "\n",
    "$n$ - the dataset size \n",
    "\n",
    "$d$ - feature dimension.\n",
    "\n",
    "$X\\in\\mathbb{R}^{n\\times d}$ - the dataset written in an $n\\times d$ matrix where the $i$-th row is the feature vector $x_i\\in\\mathbb{R}^d$ of data $i$.\n",
    "\n",
    "$Y\\in\\{0, 1\\}^n$ - the labels of all data.\n",
    "\n",
    "$\\theta\\in \\mathbb{R}^d$ - the parameters of the logistic regression model to be learned.\n",
    "\n",
    "First we will implement the sigmoid function. The input $z$ is an $n$-d vector, and the output $y$ is also an $n$-d vector such that each coordnate $y_i=Sigmoid(z_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # z and y: n-dimensional vector\n",
    "    n = z.shape\n",
    "    y = np.zeros(n)\n",
    "    \n",
    "    #==== Your code here =====\n",
    "    \n",
    "    y = 1 / (1 + np.exp(-z)) # or exp(z) / 1  + exp(z)\n",
    "    \n",
    "    #==== Your code here =====\n",
    "    \n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will implement the logistic regression model. Recall that for each sample $x_i\\in\\mathbb{R}^d$, logistic regression outputs a \"probability estimate\":\n",
    "$$\n",
    "y_i = Sigmoid(\\theta \\cdot x_i)\\in(0, 1)\n",
    "$$\n",
    "\n",
    "Given a parameter $\\theta$ and a dataset $X$, your code should output $y\\in\\mathbb{R}^d$ which contains the probability estimates for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_forward(X, Theta):\n",
    "    # This function computes the predicted probability p(y|X,Theta) given X and Theta.\n",
    "    # X: n*d\n",
    "    # Theta: d-dimensional vector\n",
    "    # y: n-dimensional vector\n",
    "    n = X.shape[0]\n",
    "    y = np.zeros(n)\n",
    "    #==== Your code here =====\n",
    "    \n",
    "    # Calculate the probability estimate\n",
    "    y = sigmoid(np.dot(X, Theta)) # input z will be n dimension vector\n",
    "    \n",
    "    #==== Your code here =====  \n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should compute the gradients. In class we formulated logistic regression as an ML estimator which maximizes the likelihood under some probabilistic assumptions. In practice, it is conventional to implement it as a minimization problem instead. In fact, logistic regression can be equivalently formulated as minimizing the \"cross entropy\" loss:\n",
    "$$\n",
    "CE(Y_i, y_i)= -Y_i\\log(y_i) - (1-Y_i)\\log(1-y_i),\n",
    "$$\n",
    "where $Y_i\\in\\{0, 1\\}$ is the true label and $y_i\\in(0, 1)$ is the predicted probability.\n",
    "\n",
    "Recall that $y_i=Sigmoid(\\theta \\cdot x_i)$ is a function of $\\theta$. The optimization objective is to minimize over $\\theta\\in\\mathbb{R}^d$:\n",
    "$$\n",
    "L(X, Y; \\theta)=\\frac{1}{n}\\sum_{i=1}^nCE(Y_i, y_i).\n",
    "$$\n",
    "\n",
    "Note that we divided the sum of cross entropy losses by $n$. Without averaging, the optimization problem does not change. However, averaging would ensure that the maginitude of gradients is scalable for datasets of different sizes, and hence is preferred. \n",
    "\n",
    "Now in the $LR\\_backward$ function, compute the gradient of $L$ with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_backward(Theta, X, Y, y):\n",
    "    # Y: true label\n",
    "    # y: predicted probability\n",
    "    # grads: gradient of the loss function, d*1\n",
    "    n = X.shape[0]\n",
    "    d = Theta.shape[0]\n",
    "    grad = np.zeros(d)\n",
    "    #==== Your code here =====\n",
    "    \n",
    "    # Derive based on likelihood function derived on class\n",
    "    # Cross entropy loss = 1 - likelihood\n",
    "    derivative = y - Y\n",
    "    grad = np.dot(derivative, X) / n\n",
    "    #==== Your code here =====\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement gradient descent. To minimize the loss function $L$, we adjust the parameter $\\theta$ by moving a tiny step towards the direction of $-\\nabla_{\\theta} L$:\n",
    "$$\n",
    "\\theta\\leftarrow \\theta-\\eta\\nabla_{\\theta} L\n",
    "$$\n",
    "where $\\eta>0$ is the learning rate. Now complete this (presumably) one-line code for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(Theta, grads, learning_rate):\n",
    "    \n",
    "    #===== Your code here ======\n",
    "    \n",
    "    Theta = Theta - learning_rate * grads\n",
    "    \n",
    "    #===== Your code here\n",
    "    \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost complete! Now let's test it on the red wine dataset. Before running your code, complete the part in which you implement a complete cycle of parameter update: \n",
    "\n",
    "First pass the dataset and parameters through your model to obtain a prediction.\n",
    "\n",
    "Then, compute gradient with respect to parameters. \n",
    "\n",
    "Finally update the parameters using gradient descent.\n",
    "\n",
    "When you test (or simply play around with) your code , you may change max_iter, initialition, and learning rate. However, the hyper-parameters we provide should work if you follow closely with the previous steps. If everything works out correctly, your test error should be close to that obtained by the built-in classifier in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11d814f40>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Preprocessing: scaling and shifting the training data so that all dimensions have mean 0 and std 1\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_red)\n",
    "X_train_scaled = scaler.transform(X_train_red)\n",
    "X_test_scaled = scaler.transform(X_test_red) # Apply the same transformation to test data\n",
    "n_train = X_train_scaled.shape[0]\n",
    "n_test = X_test_scaled.shape[0]\n",
    "\n",
    "# Introduce the bias term by appending features with 1\n",
    "X_train_scaled = np.concatenate((X_train_scaled, np.ones((n_train, 1) )), axis=1)\n",
    "X_test_scaled = np.concatenate((X_test_scaled, np.ones((n_test, 1) )), axis=1)\n",
    "\n",
    "# Start training\n",
    "(n, d) = X_train_scaled.shape\n",
    "from sklearn.metrics import accuracy_score\n",
    "max_iter = 5000\n",
    "Theta = np.random.normal(0, 0.5, d) # initialization of parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "err_trn = np.zeros(max_iter//100)\n",
    "err_tst = np.zeros(max_iter//100)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    #===== Your code here =======\n",
    "    # Parameter updates\n",
    "    pred_prob = LR_forward(X_train_scaled, Theta)\n",
    "    grad = LR_backward(Theta, X_train_scaled, y_train_red, pred_prob)\n",
    "    Theta = update_parameters(Theta, grad, learning_rate)\n",
    "    #===== Your code here =======\n",
    "    \n",
    "    if i%100==0:\n",
    "    \n",
    "        pred_tst = LR_forward(X_test_scaled, Theta) > 0.5\n",
    "        pred_trn = LR_forward(X_train_scaled, Theta) > 0.5\n",
    "\n",
    "        cost_tst = accuracy_score(pred_tst, y_test_red)\n",
    "        cost_trn = accuracy_score(pred_trn, y_train_red)\n",
    "    \n",
    "        err_tst[i//100] = 1 - cost_tst\n",
    "        err_trn[i//100] = 1 - cost_trn\n",
    "    \n",
    "plt.figure(2)\n",
    "plt.plot(err_tst,label = 'Test')\n",
    "plt.plot(err_trn, label = 'Train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing error using the model that I derive from scratch (below 0.3) is close to the result obtained using built-in classifier with sklearn (0.275)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
