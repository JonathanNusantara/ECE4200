
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm,amsfonts,latexsym,bbm,xspace,graphicx,float,mathtools,
verbatim, xcolor,bm} 
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\newcommand{\new}[1]{\textcolor{red}{#1}}
%\usepackage{psfig}
\usepackage{pgfplots}

\newcommand{\future}[1]{\textcolor{red}{#1}}

\newcommand{\hP}{\hat P}
\newcommand{\hp}{\hat p}

\newcommand{\Dk}{\Delta_k}
\newcommand{\Px}{P(x)}
\newcommand{\Qx}{Q(x)}
\newcommand{\Nx}{N_x}

\newcommand{\Py}{P(y)}
\newcommand{\Qy}{Q(y)}
\newcommand{\Pml}{P_{ML}}
\newcommand{\Pmlx}{\Pml(x)}
\newcommand{\Pbeta}{P_{\beta}}
\newcommand{\Pbetax}{\Pbeta(x)}

\newcommand{\dTV}[2]{d_{TV} (#1,#2)}
\newcommand{\dKL}[2]{D(#1||#2)}
\newcommand{\chisq}[2]{\chi^2(#1,#2)}
\newcommand{\eps}{\varepsilon}

\newcommand{\nPepsp}[1]{n^*(#1, \eps)}
\newcommand{\nPeps}{\nPepsp{\cP}}


\newcommand{\sumX}{\sum_{x\in\cX}}

\newcommand{\Bpr}[1]{Bern(#1)}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\input{glodef} 

\title{Assignment Six\\ ECE 4200}
\date{}

\begin{document}
\maketitle 


\begin{itemize}
\item
Provide credit to \textbf{any sources} other than the course staff that helped you solve the problems. This includes \textbf{all students} you talked to regarding the problems. 	
\item
You can look up definitions/basics online (e.g., wikipedia, stack-exchange, etc).
\item
{\bf The due date is 10/25/2020, 23.59.59 ET}. 
\item
Submission rules are the same as previous assignments.
\end{itemize}



\begin{problem}{1. (15 points)}
SVM's obtain \emph{non-linear} decision boundaries by mapping the feature vectors $\bar X\in \RR^d$ to a possibly high dimensional space via a function $\phi:\RR^d\to\RR^m$, and then finding a linear decision boundary in the new space. 

We also saw that to implement SVM, it suffices to know the kernel function $K(\bar X_i, \bar X_j)=\phi(\bar X_i)\cdot \phi(\bar X_j)$, without even explicitly specifying the function $\phi$. 

Recall \textbf{Mercer's theorem}. $K$ is a kernel function if and only if for any $n$ vectors, $\bar X_1\upto \bar X_n\in \RR^d$, and \textbf{any} real numbers $c_1, \ldots, c_n$, $\sum_{i=1}^n\sum_{j=1}^n c_i c_j K(\bar X_i, \bar X_j)\ge0$. 

\begin{enumerate}
\item 
Prove the following half of Mercer's theorem (which we showed in class). If $K$ is a kernel then $\sum_{i=1}^n\sum_{j=1}^n c_i c_j K(\bar X_i, \bar X_j)\ge0$. 
\item
Let $d=1$, and $x,y\in \RR$. Is the function $K(x,y)=x+y$ a kernel? 
\item 
Let $d=1$, and $x,y\in\RR$. Is $K(x,y)=xy+1$ a kernel?
\item
Suppose $d=2$, namely the original features are of the form $\bar X_i = [\bar X^1, \bar X^2]$. Show that $K(\bar X, \bar Y)=(1+\bar X\cdot \bar Y)^2$ is a kernel function. This is called as \textbf{quadratic kernel}.
 
(\textbf{Hint}: Find a $\phi:\RR^2\to \RR^m$ (for some $m$) such that $\phi(\bar X)\cdot \phi(\bar Y) = (1+\bar X\cdot \bar Y)^2$). 
\item
Consider the training examples $\langle[0, 1], 1\rangle, \langle[1, 2], 1\rangle,\langle[-1, 2], 1\rangle,\langle[0, 11], 1\rangle, \langle[3, 4], -1\rangle, \langle[-3, 4], -1\rangle,$ $ \langle[1 -1], -1\rangle, \langle[-1, -1], -1\rangle$. We have plotted the data points below.
\begin{itemize}
\item
Is the data \textbf{linearly classifiable} in the original 2-d space? 
If yes, please come up with \emph{any} linear decision boundary that separates the data.
If no, please explain why.
\item 
Is the data linearly classifiable in the feature space corresponding to the quadratic kernel. 
If yes, please come up with \emph{any} linear decision boundary that separates the data.
If no, please explain why.
\end{itemize}
\end{enumerate}

\begin{center}
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
    axis lines=middle,
    xmin=-8, xmax=8,
    ymin=-5, ymax=16,
    xtick=, ytick=
]
\addplot [only marks, mark=x, mark options={red}] table {
0 1
1 2
-1 2   
0 11
};
\addplot [only marks, mark=o, mark options={blue}] table {
3 4
-3 4
1 -1
-1 -1
};
\end{axis}
\end{tikzpicture}
\end{center}

\end{problem}

\begin{problem}{2. (10 points)}
Let $f, h_i, 1\leq i\leq n$ be real-valued functions and let $\alpha \in \mathbb{R}^n$. Let $L(z,\bm{\alpha}) = f(z) + \sum\limits_{i=1}^{n} \alpha_i h_i(z)$. In this problem, we will prove that the following two optimization problems are equivalent.

\begin{minipage}{0.45\textwidth}
\begin{equation}\label{eq:constrained_opti}
\begin{aligned}
& \min_z f(z) \\
& \text{s.t. } h_i(z) \leq 0, \; i = 1, \ldots, n.
\end{aligned}
\end{equation} 
\end{minipage}%
\hfill
\begin{minipage}{0.45\textwidth}
\begin{equation}\label{eq:minmax}
\min_z \left[\max_{\bm{\alpha} \geq \bm{0}} L(z,\bm{\alpha})\right]
\end{equation}
\end{minipage}

Let $(z^*,\alpha^*)$ be the solution of \eqref{eq:minmax} and let $z_p^*$ be the solution of \eqref{eq:constrained_opti}.  Prove that:
	$$L(z^*, \alpha^*)  = f(z_p^*)$$
\textbf{Hint}: Use the fact that for any $z$, $\bm{\alpha} \geq \bm{0}$,  $L(z^*,\alpha^*) \geq L(z^*,\alpha)$ and $L(z^*, \alpha^*) \leq L(z,\alpha_z)$, where $\alpha_z = \arg \max_{\alpha \geq \bm{0}} L(z,\alpha)$.

You may follow the following steps but it is not required as long as your proof is correct.
\begin{enumerate}
\item Prove that $L(z^*, \alpha^*) \leq f(z_p^*)$
\item Prove that $L(z^*, \alpha^*) \geq f(z_p^*)$
\end{enumerate}
\end{problem}


\begin{problem}{3. (15 points)}
In this problem, we derive the dual formulation of the soft-margin SVM problem with $\bar \xi=\xi_1, \ldots, \xi_n$.

\begin{equation}\label{eq:soft-svm}
\begin{aligned}
& \min_{\bar w, \bar \xi} \frac 12 \|\bar w\|_2^2 +C\cdot \sum_{i=1}^n \xi_i\\
& \text{such that } \\
&\qquad\qquad 1-\xi_i-y_i(\bar X_i\cdot \bar w-t) \leq 0, \; i = 1, \ldots, n,\\
&\qquad\qquad -\xi_i\le 0, \; i = 1, \ldots, n.
\end{aligned}
\end{equation} 
Now we can define $2n$ Lagrangian variables $\bm{\alpha}=\alpha_1, \ldots, \alpha_n$, and $\bm{\beta}= \beta_1, \ldots, \beta_n$ corresponding to these equations and obtain the following Lagrangian.
\begin{equation}
\begin{aligned}
L(\bar w,t,\bar \xi, \bm{\alpha}, \bm{\beta}) = \frac 12 \|\bar w\|_2^2 +C\cdot \sum_{i=1}^n \xi_i + \sum_{i=1}^n\alpha_i(1-\xi_i-y_i(\bar X_i\cdot \bar w-t))-\sum_{i=1}^n \beta_i \xi_i
\end{aligned}
\end{equation} 
The original problem is now equivalent to 
\[
\min_{\bar w, t, \bar \xi}\left[\max_{\bm{\alpha} \geq \bm{0},\bm{\beta} \geq \bm{0}}L(\bar w,t,\bar \xi, \bm{\alpha}, \bm{\beta})\right].
\]
For this objective, minmax theorem says that the min max problem is equivalent to the max min problem below. You do not have to prove this, but are encouraged to do so (using the argument given in the discussion earlier). This gives the following problem.
\[
\max_{\bm{\alpha} \geq \bm{0},\bm{\beta}\geq \bm{0}}\left[\min_{\bar w, t, \bar \xi} L(\bar w,t,\bar \xi, \bm{\alpha}, \bm{\beta})\right].
\]
\begin{enumerate}
\item 
For a fixed $\bm{\alpha}, \bm{\beta}$ take the gradient of the the Lagrangian with respect to $\bar w$, and express $\bar w$ in terms of the other variables. 
\item
Differentiate the Lagrangian with respect to $t$, and equate to zero to obtain another equation. 
\item
Differentiate the Lagrangian with respect to $\xi_i$ and show that $\alpha_i+\beta_i=C$ at the optimum. This shows that $\alpha_i\le C$, since $\beta_i\ge0$.
\item
The expressions from 1, 2, 3 define one of the KKT conditions. Show that under these conditions,
\[
 \min_{\bar w, t, \bar \xi} L(\bar w,t,\bar \xi, \bm{\alpha}, \bm{\beta}) = \sum_{i=1}^n \alpha_i - \frac 12 \sum_{i=1}^n \sum_{j=1}^n \alpha_i\alpha_j y_iy_j(\bar X_i\cdot \bar X_j).
\]
Basically, use the expressions from 1, 2, and 3 to "cancel out" $\bar w, t, \bar \xi$ in the Lagrangian.

\item 
Combine the results above to argue that the following optimization is equivalent to the soft margin SVM we started with.
\begin{equation}\label{eq:soft-svm}
\begin{aligned}
&\max_{\bm{\alpha} \geq \bm{0}} \sum_{i=1}^n \alpha_i - \frac 12 \sum_{i=1}^n \sum_{j=1}^n \alpha_i\alpha_j y_iy_j(\bar X_i\cdot \bar X_j)\\
& \text{such that } \\
&\qquad\qquad0\leq\alpha_i\leq C, \; i = 1, \ldots, n.\\
\end{aligned}
\end{equation}
\end{enumerate}

\end{problem}



\begin{problem}{4 (25 points) SVM Classification}
	Please refer to the Jupyter Notebook in the assignment, and complete the coding part in it!
	You can use sklearn SVM package: \url{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC}
\end{problem}


\end{document}