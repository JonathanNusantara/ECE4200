
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm,amsfonts,latexsym,bbm,xspace,graphicx,float,mathtools,
verbatim, xcolor} 
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\newcommand{\new}[1]{\textcolor{red}{#1}}
%\usepackage{psfig}
\usepackage{pgfplots}

\newcommand{\future}[1]{\textcolor{red}{#1}}

\newcommand{\hP}{\hat P}
\newcommand{\hp}{\hat p}

\newcommand{\Dk}{\Delta_k}
\newcommand{\Px}{P(x)}
\newcommand{\Qx}{Q(x)}
\newcommand{\Nx}{N_x}

\newcommand{\Py}{P(y)}
\newcommand{\Qy}{Q(y)}
\newcommand{\Pml}{P_{ML}}
\newcommand{\Pmlx}{\Pml(x)}
\newcommand{\Pbeta}{P_{\beta}}
\newcommand{\Pbetax}{\Pbeta(x)}


\newcommand{\dTV}[2]{d_{TV} (#1,#2)}
\newcommand{\dKL}[2]{D(#1||#2)}
\newcommand{\chisq}[2]{\chi^2(#1,#2)}
\newcommand{\eps}{\varepsilon}

\newcommand{\nPepsp}[1]{n^*(#1, \eps)}
\newcommand{\nPeps}{\nPepsp{\cP}}


\newcommand{\sumX}{\sum_{x\in\cX}}

\newcommand{\Bpr}[1]{Bern(#1)}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\input{../glodef} 

\title{Assignment Seven\\ ECE 4200}

\begin{document}
\maketitle 

\begin{itemize}
\item
Provide credit to \textbf{any sources} other than the course staff that helped you solve the problems. This includes \textbf{all students} you talked to regarding the problems. 	
\item
You can look up definitions/basics online (e.g., wikipedia, stack-exchange, etc)
\item
{\bf The due date is 11/1/2020, 23.59.59 eastern time}. 
\item
Submission rules are the same as previous assignments.
\end{itemize}



\begin{problem}{1. (10 points)}
Suppose AdaBoost is run on $n$ training examples, and suppose on each round that the weighted training error $\varepsilon_t$ of the $t$th weak hypothesis is at most $\frac12-\gamma$, for some number $\gamma>0$. Show that after $T>\frac{\ln n}{2\gamma^2}$ rounds of AdaBoost the final combined classifier has zero training error!
\end{problem}

\begin{problem}{2. (10 points)}
Recall bagging. Starting from a training set $S$ of size $n$, we created $m$ bootstrap training sets $S_1, \ldots, S_m$, each of size $n$ each by sampling with replacement from $S$.
\begin{enumerate}
\item 
For a bootstrap sample $S_i$, what is the expected fraction of the training set that does not appear at all in $S_i$? As $n\to\infty$, what does this fraction approach? 
\item
Let $m>2\ln n$, and $n\to\infty$. Show that the expected number of training examples in $S$ that appear in at least one $S_i$ is more than $n-1$. 
\end{enumerate}
\end{problem}

\begin{problem}{3. (10 points)}
The tanh function is $\tanh(y) = (e^y-e^{-y})/(e^y+e^{-y})$. Consider the function $\tanh(w_0+w_1x_1+w_2x_2)$, with five inputs, and a scalar output. 
\begin{enumerate}
\item 
Draw the computational graph of the function (you can use $\tanh$ in your computation graph).
\item
What is the derivative of $\tanh(y)$ with respect to $y$. 
\item
Suppose $(w_0, w_1, w_2, x_1, x_2) = (-2, -3, 1, 2, 3)$. Compute the forward function values, and back-propagation of gradients. 
\end{enumerate}	
\end{problem}


\begin{problem}{4. (30 points)} Please see attached notebook for details.
\end{problem}



\end{document}