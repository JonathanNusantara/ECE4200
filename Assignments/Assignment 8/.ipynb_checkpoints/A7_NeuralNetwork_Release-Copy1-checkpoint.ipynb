{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Neural Networks and Backpropagation\n",
    "In this assignment, you will be asked to write your own code to implement the learning process of a simple neural network. We will use a simple version of [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which we have introduced in Assignment 5. To make the problem simpler, we only take images with label '8' and '9', which gives us a binary classification problem. Then we subsample the dataset and reduce the dimension of each image using average pooling. The following code loads the dataset and prints its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 964)\n",
      "(196, 414)\n",
      "(964,)\n",
      "(414,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#Load data\n",
    "import scipy.io as sio\n",
    "a = sio.loadmat('mnist_binary.mat')\n",
    "X_trn = a['X_trn']\n",
    "X_tst = a['X_tst']\n",
    "Y_trn = a['Y_trn'][0]\n",
    "Y_tst = a['Y_tst'][0]\n",
    "print(X_trn.shape)\n",
    "print(X_tst.shape)\n",
    "print(Y_trn.shape)\n",
    "print(Y_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. You are not allowed to use any machine learning libraries which have neural networks implemented.\n",
    "\n",
    "2. Notice here most of the problems you have will be regarding the dimensions of variables. In each skeleton function we provide, we have one assert line to help you verify whether you write your code correctly. Passing the assert line doesn't mean your code is correct. But it is a necessary condition.\n",
    "\n",
    "3. You don't need to strictly follow the skeleton we provide. As long as you answer the problems correctly, you can write in any style you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Let's first implement a simple neural network with one hidden layer and one output layer. The hidden layer only has $n_h$ neurons. We assume the output layer has two neurons. Hence you will have 4 parameters to describe the neural network: \n",
    "\n",
    "1. $W_1$, a $n_h$ by 196 (number of features) matrix, which is the weight matrix between features and the hidder layer.\n",
    "2. $b_1$, a scalar, which is the offset for the first layer.\n",
    "3. $W_2$, a 2 by $n_h$ matrix, which is the weight matrix between the hidder layer and the output layer.\n",
    "4. $b_2$, a scalar, which is the offset for the second layer.\n",
    "\n",
    "The following script initializes the above four parameters and returns them as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (20, 196)\n",
      "b1 (20, 1)\n",
      "W2 (2, 20)\n",
      "b2 (2, 1)\n"
     ]
    }
   ],
   "source": [
    "#Initialize parameters \n",
    "num_hidden = 20 #number of neurons in the hidden layer\n",
    "num_op = 2 #number of neurons in the output layer\n",
    "\n",
    "def initialize_parameters(size_input, size_hidden, size_output):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(size_hidden, size_input) * 0.01\n",
    "    b1 = np.zeros(shape=(size_hidden, 1))\n",
    "    W2 = np.random.randn(size_output, size_hidden) * 0.01\n",
    "    b2 = np.zeros(shape=(size_output, 1))\n",
    "    parameters = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2}\n",
    "    return parameters\n",
    "parameters = initialize_parameters(X_trn.shape[0], num_hidden, num_op)\n",
    "print('W1',parameters['W1'].shape)\n",
    "print('b1',parameters['b1'].shape)\n",
    "print('W2',parameters['W2'].shape)\n",
    "print('b2',parameters['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function.\n",
    "Let $Z_2=(z_1, z_2)$ be the final otuput layer of neurons. The softmax outputs are probability estimates for outputing label 1 (assuming '8' is 1 and '9' is zero):\n",
    "\n",
    "$$\\hat{y}_1 = Pr(Y = 1 | z_1, z_2) = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}$$\n",
    "\n",
    "Write code in the cell below to do the softmax computation. Note here Z2 should be a matrix of shape $2 \\times n$, where $n$ is the number of training samples. Your output softmax should be of shape $1 \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z2):\n",
    "    # ip - (M,N) array where M is no. of neurons in output layer, N is number of samples.\n",
    "    # You can modify the code if your output layer is of different dimension\n",
    "   # =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================\n",
    "    assert(softmax.shape == (1, Z2.shape[1]))\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function.\n",
    "The following function should be able to implement activation function given the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activ(ip,act):\n",
    "    # ip - array obtained after multiplying inputs with weights (between input layer and hidden layer)\n",
    "    # act - ReLU or Sigmoid\n",
    "    if act ==\"ReLU\":\n",
    "        # =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================\n",
    "    elif act == \"Sigmoid\":\n",
    "        # =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================\n",
    "    assert(out.shape == ip.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Given $X, W_1, b_1, W_2, b_2$, the following function will compute the neurons and activated values in the hidden layer, denoted by $Z_1, A_1$ respectively. It will also return the neurons in the last layer and the softmax function computed from it, denoted by $Z_2, A_2$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Propagation   \n",
    "def forward_propagation(X, parameters, act):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    neuron = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    return neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "In this assignment, we will use the cross-entropy loss defined below as our loss function. Recall that $Z_2=(z_1, z_2)$ is the final layer of neurons, and after softmax we obtain $\\hat{y}$ which corresponds to the probability of label 8. Let $y$ be the true labels (assume 1 for '8', 0 for '9')\n",
    "$$\\ell(y,\\hat{y}) = -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y}),$$\n",
    "where $$\\hat{y} = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}.$$\n",
    "\n",
    "You have shown in the assignment that:\n",
    "$$\\frac{\\partial \\ell(y,\\hat{y})}{\\partial z_1} = \\hat{y} - y, \\frac{\\partial L(y,\\hat{y})}{\\partial z_2} = y - \\hat{y}.$$\n",
    "Given the parameters and the neuron values, we can calculate the derivative of the loss function w.r.t all the parameters $W_1, b_1, W_2, b_2$ using backward propagation. Note here, all the gradients should be of the same dimension as the corresponding parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(parameters, neuron, X, Y, act):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    A1 = neuron['A1']\n",
    "    A2 = neuron['A2']\n",
    "    Z1 = neuron['Z1']\n",
    "    Z2 = neuron['Z2']\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================\n",
    "    \n",
    "    assert(dW1.shape == W1.shape)\n",
    "    assert(dW2.shape == W2.shape)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "#print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['dW1'].shape)\n",
    "#print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['dW2'].shape)\n",
    "#print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['db1'].shape)\n",
    "#print(backprop(parameters, neuron, X_trn, Y_trn, act='Sigmoid')['db2'].shape)\n",
    "\n",
    "def cross_entropy_loss(softmax, Y):\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "# =============================================        \n",
    "    assert(loss.shape = Y.shape)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates\n",
    "Given the parameters and the gradients, we simply update the parameters by the following:\n",
    "\n",
    "$$W = W - \\eta dW$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models\n",
    "Combining the above mentioned parameters, implement the following function to learn a neural network and do inference on it. For prediction, you take the argument that gives the largest softmax output at the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def nn_model1(X_trn, X_tst, Y_trn, Y_tst, n_h, n_o, epochs, act, learning_rate):\n",
    "    #X_trn: the training set\n",
    "    #X_tst: the test set\n",
    "    #Y_trn: training labels\n",
    "    #Y_tst: test labels\n",
    "    #n_h: number of neurons in the hidden layer\n",
    "    #n_o: number of neurons in the output layer\n",
    "    #epochs: number of epochs for the training\n",
    "    #act: the activation function you choose\n",
    "    #learning_rate: a list of length epochs, which consists of the learning rate in each step\n",
    "    \n",
    "    assert(len(learning_rate) == epochs)\n",
    "    \n",
    "   # =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================    \n",
    "    #err_tst: testing error (classification error) in each epoch\n",
    "    #err_trn: training error (classification error) in each epoch\n",
    "    #loss_trn: training loss (cross entropy loss) in each epoch\n",
    "    #parameters: the final learned parameters\n",
    "    return err_tst, err_trn, loss_trn, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Verify that your code is working well.\n",
    "Using ReLU (Sigmoid) as your activation function, implement a learning algorithm with fixed learning rate $\\eta = 0.01$ at each step. Set the number of epochs to be 20000. Plot the cross entropy loss at each epoch to convince yourself that you are training well. (Your cross entropy loss should be decreasing smoothly. This part won't be graded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "lr1 = 0.01*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.plot(range(epochs), loss_trn, '-', color='orange',linewidth=2, label='training loss (lr = 0.01)')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cross entropy error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Learning with fixed learning rate.\n",
    "Using ReLU as your activation function, implement a learning algorithm with fixed learning rate $\\eta = 0.01$ at each step. Plot the training and testing error (classification error) you get at each epoch. Justify your plot. (Set the number of hidden neurons in the hidden layer to be 20 for problem 1-3, for all problems below, set epochs = 20000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "lr1 = 0.01*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "plt.figure(1, figsize=(12, 8))\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01)')\n",
    "#plt.plot(range(epochs), trn_loss, '-r', linewidth=2, label='loss (lr = 0.01)')\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.01)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: \n",
    "Using ReLU as your activation function, change the learning rate to $\\eta = 0.1$. Plot the plots on the same figure as in problem 1. Compare the plots and justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = 0.1*np.ones(epochs)\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "plt.figure(2, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01)')\n",
    "\n",
    "# Classification errors for learning rate = 0.1, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn2, '-', linewidth=2, label='training error (lr = 0.1)')\n",
    "plt.plot(range(epochs), err_tst2, '-b', color='yellow', linewidth=2,  label='test error (lr = 0.1)')\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.1)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Learning with variable learning rate.\n",
    "Using ReLU as your activation function, implement a learning algorithm with variable learning rate $\\eta = \\frac1{\\sqrt{i+1}}$ at the $i$th step. Plot the training and testing error you get at each iteration and compare it with the plots you get previously. Justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(epochs))\n",
    "lr3 = 1/np.sqrt(indices + 1)\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "plt.figure(3, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (lr = 0.01)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (lr = 0.01)')\n",
    "\n",
    "# Classification errors for learning rate = 0.1, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn2, '-', color='red', linewidth=2, label='training error (lr = 0.1)')\n",
    "plt.plot(range(epochs), err_tst2, '-b', color='yellow', linewidth=2, label='test error (lr = 0.1)')\n",
    "\n",
    "# Classification errors for variable learning rate, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn3, '-', color='purple', linewidth=2, label='training error (unfixed lr)')\n",
    "plt.plot(range(epochs), err_tst3, '-b', color='green', linewidth=2, label='test error (unfixed lr)')\n",
    "plt.title('ReLU(Variable learning rate)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Larger hidden layer.\n",
    "Change the number of neurons in the hidden layer to be $50$. Redo the experiment in problem 1. Plot all four plots in the same figure and justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden2 = 50\n",
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "plt.figure(4, figsize=(12, 8))\n",
    "# Classification errors for learning rate = 0.01, Relu Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (#hidden = 20)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (#hidden = 20)')\n",
    "\n",
    "# Classification errors for learning rate = 0.01, Relu Activation, n_hidden = 50\n",
    "plt.plot(range(epochs), err_trn4, '-', color='red', linewidth=2, label='training error (#hidden = 50)')\n",
    "plt.plot(range(epochs), err_tst4, '-b', color='grey', linewidth=2, label='test error (#hidden = 50)')\n",
    "\n",
    "plt.title('ReLU(Learning rate=0.01)')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Sigmoid Activation.\n",
    "Change the activation function to be Sigmoid function. Redo the experiment in problem 1. Plot all four plots in the same figure and justify your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========Write your code below ==============\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Classification errors for learning rate = 0.01, Relu Activation, n_hidden = 20\n",
    "plt.figure(5, figsize=(12, 8))\n",
    "plt.plot(range(epochs), err_trn, '-', color='orange',linewidth=2, label='training error (ReLU)')\n",
    "plt.plot(range(epochs), err_tst, '-b', linewidth=2, label='test error (ReLU)')\n",
    "\n",
    "# Classification errors for learning rate = 0.01, Sigmoid Activation, n_hidden = 20\n",
    "plt.plot(range(epochs), err_trn5, '-', color='red',  linewidth=2, label='training error (Sigmoid)')\n",
    "plt.plot(range(epochs), err_tst5, '-b', color='green', linewidth=2, label='test error (Sigmoid)')\n",
    "\n",
    "plt.title('Learning rate=0.01')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
