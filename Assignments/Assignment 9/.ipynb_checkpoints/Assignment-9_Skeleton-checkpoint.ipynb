{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Normalization\n",
    "\n",
    "In this assignment we will study the effect of normalization. When the ranges of different dimensions do not have the same magnitude, the learning outcome may go wrong significantly. We first start with a sythetic dataset with two dimensions. On the first dimension the data is uniform over $[-10^4, 10^4]$ and on the second dimension the data is uniform over $[-1, 1]$. Labels are assigned according to the sign of the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_train = 200\n",
    "n_test = 200\n",
    "\n",
    "# Train set\n",
    "X_train = np.random.uniform(-1, 1, [n_train, 2])\n",
    "X_train[:, 0] = 1e4 * X_train[:, 0]\n",
    "Y_train = X_train[:, 1]>0\n",
    "\n",
    "# Test set\n",
    "X_test = np.random.uniform(-1, 1, [n_test, 2])\n",
    "X_test[:, 0] = 1e4 * X_test[:, 0]\n",
    "Y_test = X_test[:, 1]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Train a linear SVM with $C=1$, k-nearest neighbor (with k=5), and a decision tree classifier using (X_train, Y_train) and test them on (X_test, Y_test). Print their test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC test accuracy:  0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# ====== Your code here ========\n",
    "clf1 = SVC(C=1, kernel='linear')\n",
    "clf1.fit(X_train, Y_train)\n",
    "print(\"SVC test accuracy: \", accuracy_score(Y_test, clf1.predict(X_test)))\n",
    "# ====== Your code here ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN test accuracy:  0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# ====== Your code here ========\n",
    "clf2 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2.fit(X_train, Y_train)\n",
    "print(\"KNN test accuracy: \",accuracy_score(Y_test, clf2.predict(X_test)))\n",
    "# ====== Your code here ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# ====== Your code here ========\n",
    "clf3 = tree.DecisionTreeClassifier()\n",
    "clf3.fit(X_train, Y_train)\n",
    "print(\"Decision tree test accuracy: \",accuracy_score(Y_test, clf3.predict(X_test)))\n",
    "# ====== Your code here ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Now we implement the normalization step. You cannot use existing tools for normalization. \n",
    "\n",
    "First we compute the mean and standard deviation of each column in X_train: for dimension $i$ let $\\mu_i$ and $\\sigma_i$ be the mean and standard deviation respectively. Then we normalize both X_train and X_test using the mean and standard deviation we just obtained, meaning that for each row $x=(x_1, x_2)$ of X_train and X_test, we compute $x'=(x_1', x_2')$ by\n",
    "$$\n",
    "x_i' = \\frac{x_i-\\mu_i}{\\sigma_i}, i=1, 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "mean = np.zeros((1, X_train.shape[1]))\n",
    "std = np.ones((1, X_train.shape[1]))\n",
    "\n",
    "\n",
    "# ====== Your code here ========\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "# ====== Your code here ========\n",
    "\n",
    "def transform(X, mean, std):\n",
    "    # X: n x d matrix\n",
    "    # mean and std: 1 x d matrix\n",
    "    # X_out: n x d matrix\n",
    "    X_out = np.zeros(X.shape)\n",
    "    # ====== Your code here ========\n",
    "   \n",
    "    X_out = (X - mean) / std\n",
    "   \n",
    "    # ====== Your code here ========\n",
    "    return X_out \n",
    "\n",
    "X_train_scaled = transform(X_train, mean, std)\n",
    "X_test_scaled = transform(X_test, mean, std)\n",
    "\n",
    "print(mean.shape)\n",
    "print(std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the same classifiers in Problem 1, except this time using X_train_scaled, Y_train. Test your models using X_test_scaled, Y_test and compare the test accuracies with Problem 1. \n",
    "1. How does normalization affect the accuracy of SVM? \n",
    "2. How does normalization affect the accuracy of k-NN?\n",
    "3. How does normalization affect the accuracy of decision tree?\n",
    "4. Are they affected similarly or differently? Briefly explain why the effects might be similar or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC test accuracy:  0.975\n",
      "KNN test accuracy:  0.955\n",
      "Decision tree test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# ====== Your code here ========\n",
    "clf1 = SVC(C=1, kernel='linear')\n",
    "clf1.fit(X_train_scaled, Y_train)\n",
    "print(\"SVC test accuracy: \", accuracy_score(Y_test, clf1.predict(X_test_scaled)))\n",
    "\n",
    "clf2 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2.fit(X_train_scaled, Y_train)\n",
    "print(\"KNN test accuracy: \",accuracy_score(Y_test, clf2.predict(X_test_scaled)))\n",
    "\n",
    "clf3 = tree.DecisionTreeClassifier()\n",
    "clf3.fit(X_train_scaled, Y_train)\n",
    "print(\"Decision tree test accuracy: \",accuracy_score(Y_test, clf3.predict(X_test_scaled)))\n",
    "# ====== Your code here ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your response:\n",
    "1. The normalization helps increase the SVM test accuracy from 0.93 to 0.975. Although it is not seem like a significant improvement, it actually brings the accuracy to be almost perfect.\n",
    "2. The normalization helps increase the KNN test accuracy from 0.51 to 0.955. This is a significant improvement in test accuracy, bringing the accuracy to be almost perfect.\n",
    "3. The normalization does not contribute anything to the Decision Tree Classifier because the test accuracy was already perfect even without normalization.\n",
    "4. SVM and K-NN were affected similarly by normalization as the testing accuracy improves after normalizing the training and testing features. On the other hand, Decision Tree was affected differently as the testing accuracy was already perfect before normalization. Normalization is one way of feature scaling in which it brings the data or features to be of the same scale. This helps models like SVM and K-NN, in which the algorithm/model checks for the similarities/distances between the points of the data/features and the model will be affected by the scale of the features. If there is a feature that is in a different scale, it may end up dominating the model and the test prediction will not be good. As a result, we can see how having the features in the same scale helps better test predictions in SVM and KNN. On the other hand, we do not really need to perform normalization when using Decision Tree Classifier as shown in our experiment above, as Decision Tree is just making comparisons down the tree and scale does not really affect the model. Additionally, we are able to predict the test data perfectly.  Nevertheless, it is usually a good idea to have our features in the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
